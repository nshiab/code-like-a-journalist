---
title: Extraction de donn√©es web üîç
description: Apprenez √† extraire des donn√©es depuis des sites et des pages web avec TypeScript.
---

import { Callout } from "nextra/components";
import { NoticeIntro, NoticeEnd } from "../../components/Notices.jsx";

# Extraction de donn√©es web üîç

Internet est une source d'information incroyable. Mais les donn√©es ne sont pas toujours facilement t√©l√©chargeables. Parfois, l'information est simplement affich√©e sur des pages web et‚Ä¶ c'est tout !

Pour la r√©cup√©rer, vous devez extraire les donn√©es directement √† partir du code HTML et, sur des sites plus complexes, vous devez automatiser ou simuler des clics sur la page pour obtenir ce que vous voulez.

Un mot d'avertissement : avant toute extraction, **assurez-vous toujours que ce que vous faites est l√©gal**. Dans certains cas, l‚Äôextraction et la copie de donn√©es sont interdites. De plus, **respectez toujours l‚Äôinfrastructure**. N‚Äôinondez pas les sites web de requ√™tes. Soyez conscient des ressources et des co√ªts que votre scraping peut engendrer pour les personnes et les organisations qui h√©bergent ces sites.

Notez que je pars du principe que vous avez compl√©t√© les sessions pr√©c√©dentes de ce cours. La section **4. Fondamentaux du web üåê** est particuli√®rement importante. Si vous ne savez pas comment une page web est construite, il vous sera tr√®s difficile d‚Äôen extraire des donn√©es.

<NoticeIntro lang="fr" />

## Configuration

Pour configurer notre projet, utilisons [setup-sda](https://github.com/nshiab/setup-sda), comme nous l'avons fait dans les le√ßons pr√©c√©dentes, mais avec l‚Äôoption `--scrape` pour installer quelques librairies suppl√©mentaires.

Cr√©ez un nouveau dossier, ouvrez-le avec VS Code et ex√©cutez `deno -A jsr:@nshiab/setup-sda --scrape`.

![Une capture d'√©cran de VS Code montrant un script simulant la bourse](/assets/web-scraping/setup.png)

## Extraction de tableaux

[Les tableaux HTML](https://www.w3schools.com/html/html_tables.asp) sont communs pour afficher des donn√©es sur les sites web.

Par exemple, sur [cette page Wikip√©dia](https://en.wikipedia.org/wiki/Medieval_demography#Demographic_tables_of_Europe's_population) sur la d√©mographie m√©di√©vale en Europe, vous pouvez voir plusieurs tableaux.

Si vous inspectez celui nomm√© *European population dynamics, years 1000‚Äì1500* dans un navigateur (clic droit sur le tableau puis *Inspecter* dans le menu qui s'ouvre), vous verrez l‚Äô√©l√©ment HTML du tableau. Si vous explorez son code, vous verrez les diff√©rents √©l√©ments qui composent ce tableau avec les donn√©es qu‚Äôil contient.

![Inspection d‚Äôune page HTML avec Chrome.](/assets/web-scraping/wikipedia-table.png)

Comme ces structures HTML sont toujours les m√™mes, j‚Äôai publi√© une fonction pour extraire les donn√©es int√©gr√©es dans des tableaux comme celui-ci dans la [librairie journalism](https://github.com/nshiab/journalism/). La librairie est install√©e automatiquement lorsque vous installez tout avec `setup-sda`.

### Avec un `index`

Vous pouvez passer √† `getHtmlTable` l‚ÄôURL que vous souhaitez utiliser. Par d√©faut, elle renverra les donn√©es du premier tableau de la page. Mais sur la page Wikip√©dia, le tableau que nous voulons est en r√©alit√© le quatri√®me dans le code HTML. Nous pouvons donc passer l‚Äôoption `{ index: 3 }`.

Copiez-collez le code ci-dessous dans `sda/main.ts` et ex√©cutez `deno task sda` dans votre terminal pour lancer et surveiller l‚Äôex√©cution.


```ts showLineNumbers filename="sda/main.ts" {2, 6-11}
import { SimpleDB } from "@nshiab/simple-data-analysis";
import { getHtmlTable } from "@nshiab/journalism";

const sdb = new SimpleDB();

const medievalData = await getHtmlTable(
  "https://en.wikipedia.org/wiki/Medieval_demography",
  { index: 3 },
);

console.table(medievalData);

await sdb.done();
```
![Un tableau Wikip√©dia extrait.](/assets/web-scraping/medieval.png)

<Callout type="info" emoji="üí°">
    Si la mise en page du tableau s'affiche √©trangement dans votre terminal, c'est parce que la largeur du tableau d√©passe celle du terminal. Faites un clic droit dans le terminal et cherchez l‚Äôoption `Toggle size with content width`. Il existe aussi un raccourci tr√®s pratique que j‚Äôutilise tout le temps pour √ßa : `OPTION` + `Z` sur Mac et `ALT` + `Z` sur PC.
</Callout>

Les donn√©es retourn√©es par `getHtmlTable` sont une liste d‚Äôobjets, ce qui signifie que vous pouvez facilement les utiliser avec la [librairie Simple Data Analysis](https://github.com/nshiab/simple-data-analysis) pour les mettre en cache et les analyser.


```ts showLineNumbers filename="sda/main.ts" {2, 6, 8, 13-14, 16-24}
import { SimpleDB } from "@nshiab/simple-data-analysis";
import { getHtmlTable } from "@nshiab/journalism";

const sdb = new SimpleDB();

const medievalDemography = sdb.newTable("mediavalDemography");

await medievalDemography.cache(async () => {
  const medievalData = await getHtmlTable(
    "https://en.wikipedia.org/wiki/Medieval_demography",
    { index: 3 },
  );
  await medievalDemography.loadArray(medievalData);
});

await medievalDemography.convert({
  Year: "number",
  "Total European population,millions": "number",
});
await medievalDemography.logTable();
await medievalDemography.logLineChart(
  "Year",
  "Total European population,millions",
);

await sdb.done();
```
![Un tableau Wikip√©dia mis en cache.](/assets/web-scraping/medieval-cache.png)
<Callout type="info" emoji="üí°">
    Mettre les donn√©es en cache est tr√®s important. Si vous ne vous attendez pas √† ce que les donn√©es changent, vous pouvez les sauvegarder sur votre ordinateur au lieu de les r√©cup√©rer encore et encore. Avec la librairie SDA, le cache est tr√®s simple √† g√©rer. La m√©thode `cache` cr√©e un dossier `.sda-cache` qui stocke les donn√©es dans votre projet. Si vous souhaitez que les donn√©es expirent apr√®s un certain temps, consultez l‚Äôoption `ttl` dans la [documentation](https://jsr.io/@nshiab/simple-data-analysis/doc/~/SimpleTable.prototype.cache). Pour en savoir plus sur la librairie SDA, consultez la section [`3. La librairie SDA ü§ì`](/tabular-data).
</Callout>

### Avec un `selector`

La fonction `getHtmlTable` peut aussi utiliser un s√©lecteur CSS pour r√©cup√©rer les donn√©es d‚Äôun tableau sp√©cifique que vous souhaitez cibler.

Par exemple, [les d√©put√©s canadiens doivent divulguer leurs d√©penses tous les trois mois](https://www.ourcommons.ca/proactivedisclosure/en/members/2024/1). Sur cette page, on peut voir que les donn√©es sont stock√©es dans un tableau avec l‚Äôidentifiant `data-table`. On peut utiliser cet identifiant directement avec notre fonction.

PS : Notez que vous pouvez t√©l√©charger les donn√©es directement au format CSV. Mais je cherchais un site public qui ne change pas trop avec le temps, et celui-ci correspond bien √† ce crit√®re !

```ts showLineNumbers filename="sda/main.ts" {6-9, 11}
import { SimpleDB } from "@nshiab/simple-data-analysis";
import { getHtmlTable } from "@nshiab/journalism";

const sdb = new SimpleDB();

const expensesData = await getHtmlTable(
  "https://www.ourcommons.ca/proactivedisclosure/en/members/2024/1",
  { selector: "#data-table" },
);

console.table(expensesData);

await sdb.done();
```
![D√©penses des d√©put√©s extraites d‚Äôun tableau web.](/assets/web-scraping/MPS-expenses.png)

Comme nous l‚Äôavons fait avec le tableau Wikip√©dia, nous pouvons mettre en cache et analyser ces donn√©es avec SDA. Ici, nous calculons les d√©penses moyennes par parti.

```ts showLineNumbers filename="sda/main.ts" {6-7, 12-13, 15-32}
import { SimpleDB } from "@nshiab/simple-data-analysis";
import { getHtmlTable } from "@nshiab/journalism";

const sdb = new SimpleDB();

const MPs = sdb.newTable("MPs");
await MPs.cache(async () => {
  const expensesData = await getHtmlTable(
    "https://www.ourcommons.ca/proactivedisclosure/en/members/2024/1",
    { selector: "#data-table" },
  );
  await MPs.loadArray(expensesData);
});

await MPs.replace(["Salaries", "Travel", "Hospitality", "Contracts"], {
  "$": "",
  ",": "",
});
await MPs.convert({
  Salaries: "number",
  Travel: "number",
  Hospitality: "number",
  Contracts: "number",
}, { try: true });
await MPs.summarize({
  values: ["Salaries", "Travel", "Hospitality", "Contracts"],
  categories: "Caucus",
  summaries: "mean",
  decimals: 0,
});
await MPs.wider("Caucus", "mean");
await MPs.logTable();

await sdb.done();
```
![D√©penses des d√©put√©s extraites et r√©sum√©es avec SDA.](/assets/web-scraping/MPS-summary.png)

## Extraction depuis des pages

### Pages simples

Les donn√©es ne sont pas toujours bien rang√©es dans des tableaux. Parfois, elles tra√Ænent un peu partout dans le code des pages web.

Par exemple, voici la [liste de tous les d√©put√©s canadiens actuellement en fonction](https://www.ourcommons.ca/Members/en/search?parliament). Cette liste change avec le temps, donc vous n‚Äôaurez peut-√™tre pas exactement les m√™mes que moi, mais ce n‚Äôest pas grave pour cette le√ßon.

![Tous les d√©put√©s actuellement en fonction.](/assets/web-scraping/all-MPS.png)

Si vous voulez conna√Ætre leur langue d'usage (le Canada est bilingue üá®üá¶), vous devez cliquer sur la page personnelle de chacun d‚Äôeux.

Comment pourrait-on r√©cup√©rer la langue pr√©f√©r√©e de tous les d√©put√©s ?

![Page personnelle d‚Äôun d√©put√©.](/assets/web-scraping/personal-page-MP.png)

Quand les donn√©es sont r√©parties sur plusieurs pages, la premi√®re √©tape consiste souvent √† rassembler toutes les URLs.

Pour r√©cup√©rer toutes les URLs, on peut utiliser [Playwright](https://github.com/microsoft/playwright). C‚Äôest un projet en code ouvert de Microsoft. Il a √©t√© cr√©√© pour automatiser les tests de sites web, mais c‚Äôest aussi un excellent outil pour l'extraction de donn√©es. Playwright permet de prendre le contr√¥le d‚Äôun navigateur web avec du code et d‚Äôextraire ce que vous voulez des pages visit√©es par votre script.

Playwright est install√© automatiquement quand vous installez tout avec `setup-sda`.

Si on inspecte les cartes sur le site web, on peut voir que la balise `a` contient le lien vers la page personnelle du d√©put√©. Toutes les balises ont la classe `ce-mip-mp-tile`.

![Inspection de la page personnelle d‚Äôun d√©put√©.](/assets/web-scraping/link-MP.png)

Commen√ßons par visiter la page et extraire les URLs. Voici ce que fait le code ci-dessous, √©tape par √©tape :
- D'abord, on importe le navigateur [`chromium`](https://www.chromium.org/chromium-projects/) depuis Playwright. Chromium est un projet en code ouvert √† la base de Google Chrome. On cr√©e un nouveau `browser`, puis un nouveau `context`, et enfin une `newPage` (lignes 1‚Äì5).
- Ensuite, on demande √† cette page d‚Äôaller sur la page des d√©put√©s (ligne 7).
- On r√©cup√®re tous les √©l√©ments avec la classe `ce-mip-mp-tile` (ligne 9).
- Puis on utilise la m√©thode `evaluateAll`, qui permet d‚Äôex√©cuter du code dans le navigateur ‚Äî tr√®s pratique pour le scraping. Ici, on l‚Äôutilise pour extraire facilement les attributs `href` et r√©cup√©rer les URLs (ligne 10).
- On affiche les URLs dans le terminal (ligne 12).
- Enfin, on ferme tout ce qui est li√© √† `chromium` (lignes 14‚Äì16).

Dans votre terminal, vous devriez voir toutes les URLs affich√©es !


```ts showLineNumbers filename="sda/main.ts"
import { chromium } from "playwright-chromium";

const browser = await chromium.launch();
const context = await browser.newContext();
const page = await context.newPage();

await page.goto("https://www.ourcommons.ca/Members/en/search?parliament");

const urls = await page.locator(".ce-mip-mp-tile")
  .evaluateAll((elements) => elements.map((a) => a.href));

console.log(urls);

await page.close();
await context.close();
await browser.close();
```
![Toutes les URLs des pages personnelles des d√©put√©s.](/assets/web-scraping/mps-urls.png)

Maintenant que nous avons toutes les URLs des pages personnelles des d√©put√©s, nous pouvons it√©rer sur chacune d‚Äôelles et extraire des informations pour chaque d√©put√©.

Si vous avez acc√®s √† une IA dans les outils de d√©veloppement de votre navigateur (comme dans Chrome) et que vous ne savez pas trop quel code √©crire, vous pouvez lui poser la question directement. C‚Äôest plut√¥t pratique. Bien s√ªr, faites attention √† ce que vous demandez : ces donn√©es sont envoy√©es sur Internet (ici, √† Google) et les r√©ponses ne sont pas toujours justes !

Par exemple, dans la capture ci-dessous, j‚Äôai inspect√© le nom du d√©put√©, puis j‚Äôai fait un clic droit dessus dans le code HTML et j‚Äôai cliqu√© sur `Ask AI`. Dans la discussion en bas, j‚Äôai demand√© : `How can I retrieve the text content with Playwright?`. Et le code fourni fonctionne !

![IA utilis√©e pour r√©cup√©rer le contenu textuel dans Playwright.](/assets/web-scraping/ask-AI.png)

Dans le code ci-dessous, on extrait le nom, le parti, la circonscription, la province ou le territoire, et bien s√ªr la langue d'usage du d√©put√©.

Ici, on parcourt toutes les pages des d√©put√©s ! Donc on s‚Äôassure d‚Äôajouter un petit d√©lai (500 ms) √† la fin de chaque it√©ration pour √©viter de surcharger le serveur du site web avec trop de requ√™tes. De nombreux sites vous bloqueront si vous les visitez trop fr√©quemment.

Dans votre terminal, vous devriez voir les donn√©es s‚Äôafficher au fur et √† mesure de l‚Äôextraction !


```ts showLineNumbers filename="sda/main.ts" {12-25}
import { chromium } from "playwright-chromium";

const browser = await chromium.launch();
const context = await browser.newContext();
const page = await context.newPage();

await page.goto("https://www.ourcommons.ca/Members/en/search?parliament");

const urls = await page.locator(".ce-mip-mp-tile")
  .evaluateAll((elements) => elements.map((a) => a.href));

for (const url of urls) {
  console.log(`\n${url}`);
  await page.goto(url);
  const name = await page.locator("h1").textContent();
  const party = await page.locator(".mip-mp-profile-caucus").textContent();
  const district = await page.locator("dd > a").textContent();
  const province = await page.locator("dl > dd:nth-child(6)").textContent();
  const language = await page.locator("dl > dd:nth-of-type(4)").textContent();

  const data = { name, party, district, province, language };
  console.log(data);

  await page.waitForTimeout(500);
}

await page.close();
await context.close();
await browser.close();
```
![Donn√©es extraites pour les d√©put√©s.](/assets/web-scraping/mps-extraction.png)

Quand j‚Äôextrais des donn√©es sur des pages comme celle-ci, j‚Äôaime g√©n√©ralement ajouter quelques √©l√©ments suppl√©mentaires au script :
- Un compteur pour estimer le temps restant.
- Un log au d√©but de chaque boucle pour savoir quel √©l√©ment est en cours de traitement.
- Une √©tape de mise en cache pour √©viter de re-t√©l√©charger les donn√©es d√©j√† extraites.


```ts showLineNumbers filename="sda/main.ts" {2-3, 14, 16-19, 21, 23-26, 37-38,  41-42}
import { chromium } from "playwright-chromium";
import { exists } from "@std/fs";
import { DurationTracker } from "@nshiab/journalism";

const browser = await chromium.launch();
const context = await browser.newContext();
const page = await context.newPage();

await page.goto("https://www.ourcommons.ca/Members/en/search?parliament");

const urls = await page.locator(".ce-mip-mp-tile")
  .evaluateAll((elements) => elements.map((a) => a.href));

const tracker = new DurationTracker(urls.length, { prefix: "Remaining: " });

for (let i = 0; i < urls.length; i++) {
  const url = urls[i];
  const id = url.split("/").pop();
  const filePath = `./sda/data/${id}.json`;

  console.log(`\nProcessing ${i + 1} of ${urls.length}: ${url}`);

  if (await exists(filePath)) {
    console.log(`File already exists: ${filePath}`);
  } else {
    tracker.start();

    await page.goto(url);
    const name = await page.locator("h1").textContent();
    const party = await page.locator(".mip-mp-profile-caucus").textContent();
    const district = await page.locator("dd > a").textContent();
    const province = await page.locator("dl > dd:nth-child(6)").textContent();
    const language = await page.locator("dl > dd:nth-of-type(4)").textContent();

    const data = { name, party, district, province, language };

    const json = JSON.stringify(data, null, 2);
    await Deno.writeTextFile(filePath, json);

    await page.waitForTimeout(500);
    tracker.log();
  }
}

await page.close();
await context.close();
await browser.close();
```

Maintenant, lorsque vous ex√©cutez le script, vous avez une meilleure id√©e de sa vitesse d‚Äôex√©cution et du temps estim√© qu‚Äôil lui reste. Et surtout, s‚Äôil plante √† un moment pour n‚Äôimporte quelle raison, vous n‚Äôavez pas besoin de tout r√©extraire. Les donn√©es pr√©c√©demment extraites ont √©t√© sauvegard√©es sous forme de fichiers JSON !

![Donn√©es extraites pour les d√©put√©s, avec plus d‚Äôinformations sur l‚Äôextraction.](/assets/web-scraping/mps-caching.png)

Par d√©faut, Playwright fonctionne en mode `headless`, ce qui signifie que le navigateur automatis√© ne s‚Äôaffiche pas. Mais si vous voulez voir votre script en action, vous pouvez passer l‚Äôoption `{ headless: false }`.


```ts showLineNumbers filename="sda/main.ts" {5}
import { chromium } from "playwright-chromium";
import { exists } from "@std/fs";
import { DurationTracker } from "@nshiab/journalism";

const browser = await chromium.launch({ headless: false });
const context = await browser.newContext();
const page = await context.newPage();

await page.goto("https://www.ourcommons.ca/Members/en/search?parliament");

const urls = await page.locator(".ce-mip-mp-tile")
  .evaluateAll((elements) => elements.map((a) => a.href));

const tracker = new DurationTracker(urls.length, { prefix: "Remaining: " });

for (let i = 0; i < urls.length; i++) {
  const url = urls[i];
  const id = url.split("/").pop();
  const filePath = `./sda/data/${id}.json`;

  console.log(`\nProcessing ${i + 1} of ${urls.length}: ${url}`);

  if (await exists(filePath)) {
    console.log(`File already exists: ${filePath}`);
  } else {
    tracker.start();

    await page.goto(url);
    const name = await page.locator("h1").textContent();
    const party = await page.locator(".mip-mp-profile-caucus").textContent();
    const district = await page.locator("dd > a").textContent();
    const province = await page.locator("dl > dd:nth-child(6)").textContent();
    const language = await page.locator("dl > dd:nth-of-type(4)").textContent();

    const data = { name, party, district, province, language };

    const json = JSON.stringify(data, null, 2);
    await Deno.writeTextFile(filePath, json);

    await page.waitForTimeout(500);
    tracker.log();
  }
}

await page.close();
await context.close();
await browser.close();
```

Et maintenant, vous voyez votre code en action ! Cela peut √™tre tr√®s utile pour d√©boguer un script qui ne fonctionne pas comme pr√©vu.

![Playwright avec le mode headless d√©sactiv√©.](/assets/web-scraping/mps-browser.gif)

Enfin, vous pouvez charger tous ces fichiers JSON avec SDA et traiter vos donn√©es. Par exemple, ici, une fois l‚Äôextraction termin√©e, on compte le nombre de d√©put√©s selon leur langue d'usage.

```ts showLineNumbers filename="sda/main.ts" {4, 50-60}
import { chromium } from "playwright-chromium";
import { exists } from "@std/fs";
import { DurationTracker } from "@nshiab/journalism";
import { SimpleDB } from "@nshiab/simple-data-analysis";

const browser = await chromium.launch();
const context = await browser.newContext();
const page = await context.newPage();

await page.goto("https://www.ourcommons.ca/Members/en/search?parliament");

const urls = await page.locator(".ce-mip-mp-tile")
  .evaluateAll((elements) => elements.map((a) => a.href));

const tracker = new DurationTracker(urls.length, { prefix: "Remaining: " });

for (let i = 0; i < urls.length; i++) {
  const url = urls[i];
  const id = url.split("/").pop();
  const filePath = `./sda/data/${id}.json`;

  console.log(`\nProcessing ${i + 1} of ${urls.length}: ${url}`);

  if (await exists(filePath)) {
    console.log(`File already exists: ${filePath}`);
  } else {
    tracker.start();

    await page.goto(url);
    const name = await page.locator("h1").textContent();
    const party = await page.locator(".mip-mp-profile-caucus").textContent();
    const district = await page.locator("dd > a").textContent();
    const province = await page.locator("dl > dd:nth-child(6)").textContent();
    const language = await page.locator("dl > dd:nth-of-type(4)").textContent();

    const data = { name, party, district, province, language };

    const json = JSON.stringify(data, null, 2);
    await Deno.writeTextFile(filePath, json);

    await page.waitForTimeout(500);
    tracker.log();
  }
}

await page.close();
await context.close();
await browser.close();

const sdb = new SimpleDB();

const MPs = sdb.newTable("MPs");
await MPs.loadData("sda/data/*.json");
await MPs.summarize({
  categories: ["party", "language"],
});
await MPs.wider("party", "count");
await MPs.logTable();

await sdb.done();
```
![Chargement et r√©sum√© des donn√©es extraites sur les d√©put√©s.](/assets/web-scraping/mps-sda.png)

F√©licitations ! Vous savez maintenant comment utiliser Playwright pour extraire des donn√©es de pages web !

### Pages complexes

Certains sites web sont plus compliqu√©s √† extraire. Il n‚Äôest pas toujours possible de r√©cup√©rer simplement une liste d‚ÄôURLs. Parfois, il faut cliquer sur plusieurs menus et boutons pour acc√©der aux donn√©es souhait√©es.

Dans l‚Äôexemple ci-dessous, nous allons explorer le site d‚Äô√âlections Canada. Vous pourriez t√©l√©charger toutes les donn√©es assez facilement, mais cette interface publique est une excellente occasion d‚Äôapprendre √† extraire des donn√©es depuis des pages web plus complexes.

Disons que nous voulons r√©cup√©rer les d√©penses d√©clar√©es des candidats aux √©lections f√©d√©rales canadiennes. Il faut cliquer sur plusieurs options sur la [page d‚Äôaccueil](https://www.elections.ca/WPAPPS/WPF/EN/Home/Index).

![S√©lection de plusieurs options sur le site d‚Äô√âlections Canada.](/assets/web-scraping/EC-home.png)

Cela nous am√®ne ensuite √† une nouvelle page, avec encore de nombreuses options √† choisir.

![S√©lection d‚Äôautres options sur le site d‚Äô√âlections Canada.](/assets/web-scraping/EC-more-options.png)

Et enfin, nous avons acc√®s aux donn√©es, mais il faut encore parcourir un menu pour extraire les donn√©es de chaque candidat.

![Donn√©es des candidats sur le site d‚Äô√âlections Canada.](/assets/web-scraping/EC-candidates-data.png)

Heureusement, c‚Äôest assez facile √† faire avec les m√©thodes `selectOption` et `click` de Playwright !

Le code ci-dessous utilise l‚Äôoption `{ headless: false }`, plusieurs appels √† `await page.waitForTimeout(500)`, et un `scrollIntoViewIfNeeded` pour que vous puissiez voir le script interagir avec la page.

```ts showLineNumbers filename="sda/main.ts"
import { chromium } from "playwright-chromium";

const browser = await chromium.launch({ headless: false });
const context = await browser.newContext();
const page = await context.newPage();

await page.goto("https://www.elections.ca/WPAPPS/WPF/EN/Home/Index");
await page.selectOption("#actsList", "CC_C76");
await page.waitForTimeout(500);
await page.selectOption("#CCEventList", "53");
await page.waitForTimeout(500);
await page.selectOption("#reportTypeList", "8");
await page.waitForTimeout(500);
await page.click("#SearchButton");

await page.click("#ReturnStatusList2");
await page.waitForTimeout(500);
await page.click("#ReportOptionList1");
await page.waitForTimeout(500);
await page.click("#button3");
await page.waitForTimeout(500);
await page.click("#SelectAllCandidates");
await page.waitForTimeout(500);
await page.click("#SearchSelected");

const selectElement = page.locator("#SelectedClientId");
const optionCount = await selectElement.locator("option").count();

for (let i = 0; i < optionCount; i++) {
  const optionValue = await selectElement.locator("option").nth(i)
    .getAttribute("value");

  await selectElement.selectOption(optionValue);
  await page.click("#ReportOptions");

  await page.locator("#sumrpt").scrollIntoViewIfNeeded();

  await page.waitForTimeout(500);
}

await page.close();
await context.close();
await browser.close();
```
![It√©ration sur les candidats sur le site d‚Äô√âlections Canada.](/assets/web-scraping/complex-website.gif)

Maintenant que nous sommes capables d‚Äôit√©rer sur les candidats, nous pouvons extraire les donn√©es du tableau, comme le nom du candidat, la circonscription, le parti et les d√©penses. Nous pouvons aussi mesurer la dur√©e de l‚Äôextraction et mettre les donn√©es en cache.

Pour √©viter les probl√®mes, assurez-vous de vider votre dossier `sda/data` avant d‚Äôex√©cuter ce nouveau script.

```ts showLineNumbers filename="sda/main.ts" {2-3, 24, 27, 31, 33-36, 40-69, 72-73}
import { chromium } from "playwright-chromium";
import { exists } from "@std/fs";
import { DurationTracker } from "@nshiab/journalism";

const browser = await chromium.launch({ headless: false });
const context = await browser.newContext();
const page = await context.newPage();

await page.goto("https://www.elections.ca/WPAPPS/WPF/EN/Home/Index");
await page.selectOption("#actsList", "CC_C76");
await page.selectOption("#CCEventList", "53");
await page.selectOption("#reportTypeList", "8");
await page.click("#SearchButton");

await page.click("#ReturnStatusList2");
await page.click("#ReportOptionList1");
await page.click("#button3");
await page.click("#SelectAllCandidates");
await page.click("#SearchSelected");

const selectElement = page.locator("#SelectedClientId");
const optionCount = await selectElement.locator("option").count();

const tracker = new DurationTracker(optionCount, { prefix: "Remaining: " });

for (let i = 0; i < optionCount; i++) {
  tracker.start();
  const optionValue = await selectElement.locator("option").nth(i)
    .getAttribute("value");

  const path = `sda/data/${optionValue}.json`;

  if (await exists(path)) {
    console.log(`File already exists: ${path}`);
  } else {
    console.log(`\nRetrieving ${optionValue} (${i + 1}/${optionCount})`);
    await selectElement.selectOption(optionValue);
    await page.click("#ReportOptions");

    const name = await page.textContent("#ename1");
    if (name === null) {
      throw new Error("name is null");
    }
    const partyAndDIstrict = await page.textContent("#partydistrict1");
    if (partyAndDIstrict === null) {
      throw new Error("partyAndDIstrict is null");
    }
    const party = partyAndDIstrict.split("/")[0].trim();
    const district = partyAndDIstrict.split("/")[1].trim();
    const expenses = await page.textContent(
      "#sumrpt > tbody > tr:nth-child(16) > td > span",
    );

    console.log({
      name,
      party,
      district,
      expenses,
    });

    await Deno.writeTextFile(
      path,
      JSON.stringify([{
        name,
        party,
        district,
        expenses,
      }]),
    );

    await page.waitForTimeout(500);
    tracker.log();
  }
}

await page.close();
await context.close();
await browser.close();
```
![Mise en cache des donn√©es des candidats sur le site d‚Äô√âlections Canada.](/assets/web-scraping/caching-EC.png)

Et enfin, comme d‚Äôhabitude, on peut utiliser SDA pour charger toutes les donn√©es extraites et les analyser. Par exemple, on pourrait calculer les d√©penses moyennes par parti.


```ts showLineNumbers filename="sda/main.ts" {4, 81-92}
import { chromium } from "playwright-chromium";
import { exists } from "@std/fs";
import { DurationTracker } from "@nshiab/journalism";
import { SimpleDB } from "@nshiab/simple-data-analysis";

const browser = await chromium.launch({ headless: false });
const context = await browser.newContext();
const page = await context.newPage();

await page.goto("https://www.elections.ca/WPAPPS/WPF/EN/Home/Index");
await page.selectOption("#actsList", "CC_C76");
await page.selectOption("#CCEventList", "53");
await page.selectOption("#reportTypeList", "8");
await page.click("#SearchButton");

await page.click("#ReturnStatusList2");
await page.click("#ReportOptionList1");
await page.click("#button3");
await page.click("#SelectAllCandidates");
await page.click("#SearchSelected");

const selectElement = page.locator("#SelectedClientId");
const optionCount = await selectElement.locator("option").count();

const tracker = new DurationTracker(optionCount, { prefix: "Remaining: " });

for (let i = 0; i < optionCount; i++) {
  tracker.start();
  const optionValue = await selectElement.locator("option").nth(i)
    .getAttribute("value");

  const path = `sda/data/${optionValue}.json`;

  if (await exists(path)) {
    console.log(`File already exists: ${path}`);
  } else {
    console.log(`\nRetrieving ${optionValue} (${i + 1}/${optionCount})`);
    await selectElement.selectOption(optionValue);
    await page.click("#ReportOptions");

    const name = await page.textContent("#ename1");
    if (name === null) {
      throw new Error("name is null");
    }
    const partyAndDIstrict = await page.textContent("#partydistrict1");
    if (partyAndDIstrict === null) {
      throw new Error("partyAndDIstrict is null");
    }
    const party = partyAndDIstrict.split("/")[0].trim();
    const district = partyAndDIstrict.split("/")[1].trim();
    const expenses = await page.textContent(
      "#sumrpt > tbody > tr:nth-child(16) > td > span",
    );

    console.log({
      name,
      party,
      district,
      expenses,
    });

    await Deno.writeTextFile(
      path,
      JSON.stringify([{
        name,
        party,
        district,
        expenses,
      }]),
    );

    await page.waitForTimeout(100);
    tracker.log();
  }
}

await page.close();
await context.close();
await browser.close();

const sdb = new SimpleDB();
const returns = sdb.newTable("returns");
await returns.loadData("sda/data/*.json");
await returns.convert({ expenses: "number" }, { try: true });
await returns.summarize({
  values: "expenses",
  categories: "party",
  summaries: ["mean", "count"],
  decimals: 0,
});
await returns.sort({ mean: "desc" });
await returns.logTable(13);
```
![R√©sum√© des d√©penses des candidats avec SDA.](/assets/web-scraping/sda-EC.png)

## Extraction via des APIs non document√©es

Parfois, au lieu d‚Äôextraire √† partir du code HTML, vous pouvez utiliser directement l‚ÄôAPI qui alimente la page. Par exemple, le site de [Yahoo Finance](https://finance.yahoo.com/) affiche beaucoup de donn√©es qu‚Äôon pourrait extraire avec Playwright. Mais une autre technique consiste √† rep√©rer l‚ÄôAPI que la page appelle pour obtenir ses donn√©es.

<Callout type="info" emoji="üí°">
    API signifie *Application Programming Interface*. Sur le web, les API sont souvent utilis√©es pour transf√©rer des donn√©es. Lorsque vous appelez un point d'acc√®s API (via une URL et parfois des param√®tres), l'API renvoie les donn√©es correspondantes. Les API sont tr√®s utiles pour les sites affichant des donn√©es en temps r√©el, entre autres. Au lieu de reconstruire et republier le site avec de nouvelles donn√©es ‚Äî ce qui peut √™tre lent et co√ªteux ‚Äî il suffit de mettre √† jour les points d'acc√®s de l'API. Les r√©ponses des API sont souvent en JSON, mais elles peuvent aussi √™tre en CSV, XML et d'autres formats.
</Callout>

Sur la [page d‚Äôaccueil](https://finance.yahoo.com/), vous pouvez chercher une entreprise cot√©e en bourse. Par exemple, recherchez Apple et cliquez sur le r√©sultat correspondant.

![Une capture d‚Äô√©cran montrant le site Yahoo Finance.](/assets/web-scraping/search-apple.png)

Vous arriverez sur la page boursi√®re d‚ÄôApple. Sur la gauche, cliquez sur *Historical Data*.

![Une capture d‚Äô√©cran montrant le site Yahoo Finance.](/assets/web-scraping/apple-page.png)

Ces donn√©es ne sortent pas de nulle part. Elles viennent d‚Äôune API qui alimente la page. Jetons un coup d'≈ìil sous le capot pour en trouver la source. üßê

![Une capture d‚Äô√©cran montrant le site Yahoo Finance.](/assets/web-scraping/all-data.png)

Note : j‚Äôutiliserai Google Chrome pour les √©tapes suivantes, mais vous pouvez faire la m√™me chose avec Firefox ou Safari.

Ouvrez les *Outils de d√©veloppement* et cliquez sur l‚Äôonglet *Network*.

![Une capture d‚Äô√©cran montrant le site Yahoo Finance avec les outils de d√©veloppement ouverts.](/assets/web-scraping/network.png)

Cet onglet affiche toutes les requ√™tes faites par la page. Lorsqu‚Äôelle se charge, elle a besoin de diverses ressources comme des polices, images, styles‚Ä¶ et des donn√©es ! Toutes ces requ√™tes sont list√©es ici, et vous pouvez les explorer.

Dans notre cas, on s‚Äôint√©resse aux donn√©es boursi√®res d‚ÄôApple affich√©es dans un tableau sur la page.

Rafra√Æchissez la page, puis s√©lectionnez √† nouveau l‚Äôoption **Max** pour r√©cup√©rer toutes les donn√©es disponibles. Cherchez une requ√™te contenant `AAPL`, le symbole boursier d‚ÄôApple. C‚Äôest aussi le symbole utilis√© dans l‚ÄôURL de la page, donc c‚Äôest un bon indice.

Vous remarquerez une ou plusieurs requ√™tes `fetch` qui commencent par `AAPL`. √áa semble tr√®s prometteur !

![Une capture d‚Äô√©cran montrant le site Yahoo Finance avec les requ√™tes r√©seau d√©taill√©es.](/assets/web-scraping/appl-request.png)

Faites un clic droit sur l‚Äôune d‚Äôelles et ouvrez-la dans un nouvel onglet. Wow ! Vous reconnaissez cette syntaxe ? C‚Äôest du JSON ! Et il y a beaucoup de donn√©es. üòè

Voici [le lien](https://query1.finance.yahoo.com/v8/finance/chart/AAPL?events=capitalGain%7Cdiv%7Csplit&formatted=true&includeAdjustedClose=true&interval=1d&period1=345479400&period2=1738778777&symbol=AAPL&userYfid=true&lang=en-CA&region=CA) au cas o√π vous en auriez besoin.

![Une capture d‚Äô√©cran montrant l‚ÄôAPI de Yahoo Finance.](/assets/web-scraping/raw-data.png)

Si vous regardez de pr√®s l‚ÄôURL, vous remarquerez des param√®tres comme `symbol`, `interval`, `period1` et `period2`. Il y a aussi des param√®tres `region` et `lang`, qui peuvent varier selon votre emplacement.

`https://query1.finance.yahoo.com/v8/finance/chart/AAPL?events=capitalGain%7Cdiv%7Csplit&formatted=true&includeAdjustedClose=true&interval=1d&period1=345479400&period2=1738778777&symbol=AAPL&userYfid=true&lang=en-CA&region=CA`

Cela signifie que vous pouvez appeler cette URL pour extraire les donn√©es de Yahoo Finance, simplement en changeant les param√®tres. Et beaucoup de sites web fonctionnent de cette mani√®re, via une API.

Notez que cet exemple vient du projet [Simulateur boursier üìà](/web-scraping). Allez le voir si vous voulez en savoir plus sur l‚Äôutilisation d‚ÄôAPIs non document√©es dans vos projets.

## Conclusion

Quel parcours ! Nous avons couvert beaucoup de choses dans cette le√ßon et j‚Äôesp√®re que vous l‚Äôavez trouv√©e utile. Le web scraping est une comp√©tence essentielle pour r√©colter des donn√©es, surtout pour les journalistes computationnels.

Mais souvenez-vous : assurez-vous de toujours respecter les lois en vigueur, et ne mettez pas trop de pression sur les serveurs h√©bergeant les donn√©es qui vous int√©ressent.

Bon scraping ! ü§†

<NoticeEnd lang="fr"/>
