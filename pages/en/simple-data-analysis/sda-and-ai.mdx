---
title: Data analysis with AI ü§ñ
description: To do.
---

import { Callout } from "nextra/components";
import { NoticeIntro, NoticeEnd } from "../../../components/Notices.jsx";

# Simple Data Analysis and AI ü§ñ

Welcome to this new project in which we will use AI (more specifically large language models like ChatGPT, Gemini and Claude) to help us analyze data! These models are amazingly good at extracting, reformatting, cleaning and categorizing data.

Back in March 2025, I attended a great session at [NICAR](https://www.ire.org/training/conferences/nicar-2025/) by [Ben Welsh](https://github.com/palewire) and [Derek Willis](https://thescoop.org/about/). They showed us how to categorize claim expenses with these fascinating tools.

[Their project](https://palewi.re/docs/first-llm-classifier) (in Python) was eye opening. So I decided to implement new methods in the TypeScript library Simple Data Analysis (SDA) ([give it a ‚≠ê](https://github.com/nshiab/simple-data-analysis)!) to make great use of LLMs. I'll show you how to use these methods with a Google AI Studio API key, but they work with Vertex AI and Ollama too.

Ben nicely agreed to let me reproduce his tutorial, but this time in TypeScript! We will use Deno and VS Code. Check the [Setup](/first-steps/setup) lesson if needed. But you can do that with the JS/TS runtime and code editor of your choice! 

If you get stuck at any point in this project, it might be helpful to review the previous lessons explaining the basics of SDA:
- [Tabular data](/simple-data-analysis/tabular-data)
- [Geospatial data](/simple-data-analysis/geospatial-data)
- [Visualizing data](/simple-data-analysis/dataviz)

Let's code!

<NoticeIntro />

## Setup

To set up everything we need, let's use [setup-sda](https://jsr.io/@nshiab/setup-sda) as in previous lessons.

Create a new folder, open it with VS Code, and run: `deno -A jsr:@nshiab/setup-sda`

![A screenshot of VS Code after running setup-sda.](/assets/simple-data-analysis/sda-and-ai/setup.png)

## Getting an API key

In this lesson, we are going to use the Google LLMs. To interact with them with our code, we need an API key. Go to [Google AI Studio](https://aistudio.google.com/), create an account if you don't already have one, and click on `Get API key`.

![A screenshot of Google AI Studio.](/assets/simple-data-analysis/sda-and-ai/google-ai-studio.png)

Then click on `Create an API key`. I don't know why my page is half in French but it should work anyway. üòÖ

Follow the suggested steps. If asked about using a Google Cloud project, select Gemini if available otherwise create a new one.

![A screenshot of Google AI Studio.](/assets/simple-data-analysis/sda-and-ai/create-key.png)

Once you copied your new API key, create a new `.env` file in your project and paste it in it. Also add the model we will use, which is `gemini-2.0-flash-lite`.

`.env` is a file we usually use for **environment variable**, which are often credentials that should not be shared. When you setup everything with `setup-sda`, this file is automatically added to `.gitignore`, so it won't be committed by Git and won't be pushed to GitHub.

![Environment variables in VS Code.](/assets/simple-data-analysis/sda-and-ai/env.png)

Now, we just need to load these variables in memory.

Install the `@std/dotenv` library from the Deno team by running `deno add jsr:@std/dotenv`.

Then add `import "@std/dotenv/load";` at the top of your `sda/main.ts` file. Now, everytime you run this code, Deno will automatically load the variables in your `.env` file!

And we are ready to go! üöÄ

![A screenshot showing how to load environment variables.](/assets/simple-data-analysis/sda-and-ai/load-env.png)

## About this API...

Before we actually run some code, there is something I want to show you.

In Google AI Studio, after clicking on `Get API key`, you have acces to your `API Plan Billing`. Select the model we are going to use: `Gemini 2.0 Flash Lite`.

In there, you'll see that you have a free account that allow you to make 30 requests per minute with this model (maybe that changed since I wrote this).

And you'll also notice that... the data you are going to send will be used by Google! So be extra careful: **don't send sensitive data throught this API!**

As you might already know, nothing is really free in life... ü•≤

If you have a pro or enterprise account, they say they won't used your data to improve their products. But if you need total privacy, I'll show you at the end how to use Ollama to run models locally on your machine.

![Google AI plan billing.](/assets/simple-data-analysis/sda-and-ai/plan-billing.png)

## The data

The goal of the project is to categorize claim expenses. We are going to use the CSV sample data from Ben's tutorial. [It's hosted on GitHub](https://raw.githubusercontent.com/palewire/first-llm-classifier/refs/heads/main/_notebooks/sample.csv) and contains 250 rows.

Ben already categorized the `payee` column. We will ask our model to do the same exercice and then we will compare the human and AI results.

![The sample data.](/assets/simple-data-analysis/sda-and-ai/sample-data.png)

In the code below, we create a new table `data` and we cache the results of `loadData`, which fetches the data from GitHub. Since the data won't change, caching it makes sense. It will be stored locally in the `.sda-cache` folder. When you set everything up with `setup-sda`, this hidden folder is added to your `.gitignore`.

We also log the table to the terminal.

```ts showLineNumbers filename="sda/main.ts" {6-13}
import "@std/dotenv/load";
import { SimpleDB } from "@nshiab/simple-data-analysis";

const sdb = new SimpleDB();

const data = sdb.newTable("data");
await data.cache(async () => {
  await data.loadData(
    "https://raw.githubusercontent.com/palewire/first-llm-classifier/refs/heads/main/_notebooks/sample.csv",
  );
});

await data.logTable();

await sdb.done();
```

![Loading data from GitHub.](/assets/simple-data-analysis/sda-and-ai/loading-data.png)

## Sending data to the model

To send data and instructions to the model, we need to use the `aiRowByRow` method on our table. If curious, you'll find [the documentation here](https://jsr.io/@nshiab/simple-data-analysis/doc/~/SimpleTable.prototype.aiRowByRow).

It's usage is pretty simple:
- First, pass the column containing the data you want to clean, format, or extract data from (here it's `payee`)
- Then give the name of the new column that will be created to store the AI results (here we name it `categoryAI`)
- And finally type in your prompt. Here, we ask the LLM to categorize the `payee`.

The fourth argument is optional. It's an object with a few options:
- `rateLimitPerMinute` will ensure we respect our API quota. If we send requests too fast, the method will wait the necessary duration to avoid request being rejected
- `verbose` will log extra information while processing the data

```ts showLineNumbers filename="sda/main.ts" {13-21}
import "@std/dotenv/load";
import { SimpleDB } from "@nshiab/simple-data-analysis";

const sdb = new SimpleDB();

const data = sdb.newTable("data");
await data.cache(async () => {
  await data.loadData(
    "https://raw.githubusercontent.com/palewire/first-llm-classifier/refs/heads/main/_notebooks/sample.csv",
  );
});

await data.aiRowByRow(
  "payee",
  "categoryAI",
  `Categorize the payee into one of the following categories: Restaurant, Bar, Hotel or Other. `,
  {
    rateLimitPerMinute: 30,
    verbose: true,
  },
);

await data.logTable();

await sdb.done();
```

If you run this code, you'll see your prompt plus extra instruction added by the method.

The `verbose` option is adding useful extra information. The tokens represent the amount information sent to and received from the AI. Based on it, the method gives you an estimated cost for the request if you have a pay-as-go account.

Righ now, the data is being classified by the AI model, but... it's a bit slow.

Because we have 250 rows of data, and we can send only 30 requests per minute, classifying the whole dataset will take... over 8 minutes!

We can surely speed this up!

![Sending one row at the time.](/assets/simple-data-analysis/sda-and-ai/one-row-at-the-time.png)

## Speeding up the process

### Batches

Instead of sending one row at the time, we could send a batch of them!

In the code below, we use the option `batchSize` to send 10 rows at the time. Now, processing the whole dataset takes less than a minute!

But we can push this a bit further...

![Sending batchs of data.](/assets/simple-data-analysis/sda-and-ai/batches.png)

### Concurrency

One of the many amazing features of TypeScript and JavaScript is **concurrency**. Instead of making one request to the API at the time, you send a lot them at the same time and then wait for the results in parallel.

And taking advantage of this feature of the language can provide an incredible speed boost to our code!

Since we have 250 rows of data, we could send batches of 17 rows with 15 concurrent requests to process all of our data in less than... 2 seconds!

These 15 requests are keeping us below the quota limit of 30, which is important.

One advice: to run this code, make sure you haven't sent request for at least a minute, otherwise you might it the quota limit of the free plan.

```ts showLineNumbers filename="sda/main.ts" /{ logDuration: true }/ {18-19}
import "@std/dotenv/load";
import { SimpleDB } from "@nshiab/simple-data-analysis";

const sdb = new SimpleDB({ logDuration: true });

const data = sdb.newTable("data");
await data.cache(async () => {
  await data.loadData(
    "https://raw.githubusercontent.com/palewire/first-llm-classifier/refs/heads/main/_notebooks/sample.csv",
  );
});

await data.aiRowByRow(
  "payee",
  "categoryAI",
  `Categorize the payee into one of the following categories: Restaurant, Bar, Hotel or Other. `,
  {
    batchSize: 17,
    concurrent: 15,
    rateLimitPerMinute: 30,
    verbose: true,
  },
);

await data.logTable();

await sdb.done();
```

![Sending concurrent requests.](/assets/simple-data-analysis/sda-and-ai/concurrency.png)

### Caching

There is one last trick we can use to make our more efficient: **caching**.

We could store the result sent by the AI in files locally. If we rerun our code and if the prompt/data have not changed, we could use what we stored on our machine instead of waiting again for the AI model.

This is exactly what the `cache` option allow you to do.

```ts showLineNumbers filename="sda/main.ts" {20}
import "@std/dotenv/load";
import { SimpleDB } from "@nshiab/simple-data-analysis";

const sdb = new SimpleDB({ logDuration: true });

const data = sdb.newTable("data");
await data.cache(async () => {
  await data.loadData(
    "https://raw.githubusercontent.com/palewire/first-llm-classifier/refs/heads/main/_notebooks/sample.csv",
  );
});

await data.aiRowByRow(
  "payee",
  "categoryAI",
  `Categorize the payee into one of the following categories: Restaurant, Bar, Hotel or Other. `,
  {
    batchSize: 17,
    concurrent: 15,
    cache: true,
    rateLimitPerMinute: 30,
    verbose: true,
  },
);

await data.logTable();

await sdb.done();
```

If you run this code, you'll notice a new hidden folder `.journalism-cache`. The files in it are JSON files storing the AI responses. It works like the previous `.sda-cache` folder, but is created by the function [`askAI`](https://jsr.io/@nshiab/journalism/doc/~/askAI) from the [`journalism`](https://jsr.io/@nshiab/journalism) library, which this method `aiRowByRow` is using to interact with the AI model.

On the first run, the requests are made and the script takes the same time to run as before.

![Caching the data.](/assets/simple-data-analysis/sda-and-ai/caching.png)

But if you rerun this script, it's now way faster! It takes only a few milliseconds! Instead of making the requests, the code retrieves the stored data on your machine.

Of course, if you change the prompt or the data sent to the model, the method will know it has to make new requests and store new results. But if you don't touch the `aiRowByRow` arguments, you can safely work on other things without burning your API quotas and at full speed!

![Retrieving from the cache.](/assets/simple-data-analysis/sda-and-ai/retrieving-from-cache.png)

## Testing the output

### Tests

While they getting better at an incredible pace, LLMs don't always understand properly your instructions. And when you are making dozens, hundreds or thousands of requests, there is always a chance they could return something unexpected.

This is why testing is important.

In the code below, we use a `test` (a simple function), to check if the returned category for each falls into the expected categories.

If there is an unexpected category, we throw an error and with option `retry`, we specify that we want to retry five times before giving up and stopping everything.

```ts showLineNumbers filename="sda/main.ts" {21-29}
import "@std/dotenv/load";
import { SimpleDB } from "@nshiab/simple-data-analysis";

const sdb = new SimpleDB({ logDuration: true });

const data = sdb.newTable("data");
await data.cache(async () => {
  await data.loadData(
    "https://raw.githubusercontent.com/palewire/first-llm-classifier/refs/heads/main/_notebooks/sample.csv",
  );
});

await data.aiRowByRow(
  "payee",
  "categoryAI",
  `Categorize the payee into one of the following categories: Restaurant, Bar, Hotel or Other. `,
  {
    batchSize: 17,
    concurrent: 15,
    cache: true,
    test: (dataPoint) => {
      if (typeof dataPoint !== "string") {
        throw new Error("Not a string");
      }
      if (!["Restaurant", "Bar", "Hotel", "Other"].includes(dataPoint)) {
        throw new Error("Already categorized");
      }
    },
    retry: 5,
    rateLimitPerMinute: 30,
    verbose: true,
  },
);

await data.logTable();

await sdb.done();
```

On my end, no errors have been thrown. This is looking good!

![Testing the results.](/assets/simple-data-analysis/sda-and-ai/testing.png)

### Accuracy

Since we tested the results, we know we have the proper categories, but have they been properly attributed?

It's time to check the accuracy of the model!

Since Ben already classified the payee in the column `category` and we put the AI results in the column `categoryAI`, let's use a few SDA methods to do that!

First, let's create a new column to check if the AI returned the correct answer, assuming Ben was right.

Using this column, we can then log all occurences of different answers betwen Ben and the AI.

We can also calculate the proportion of correct answer from the AI.

```ts showLineNumbers filename="sda/main.ts" {35-45}
import "@std/dotenv/load";
import { SimpleDB } from "@nshiab/simple-data-analysis";

const sdb = new SimpleDB({ logDuration: true });

const data = sdb.newTable("data");
await data.cache(async () => {
  await data.loadData(
    "https://raw.githubusercontent.com/palewire/first-llm-classifier/refs/heads/main/_notebooks/sample.csv",
  );
});

await data.aiRowByRow(
  "payee",
  "categoryAI",
  `Categorize the payee into one of the following categories: Restaurant, Bar, Hotel or Other. `,
  {
    batchSize: 17,
    concurrent: 15,
    cache: true,
    test: (dataPoint) => {
      if (typeof dataPoint !== "string") {
        throw new Error("Not a string");
      }
      if (!["Restaurant", "Bar", "Hotel", "Other"].includes(dataPoint)) {
        throw new Error("Already categorized");
      }
    },
    retry: 5,
    rateLimitPerMinute: 30,
    verbose: true,
  },
);

await data.addColumn("correctCategory", "boolean", `category === categoryAI`);
await data.logTable({
  conditions: `correctCategory === false`,
  nbRowsToLog: "all",
});

await data.summarize({
  categories: "correctCategory",
});
await data.proportionsVertical("count", "perc");
await data.logTable();

await sdb.done();
```

And... here's the result!

If we check the rows with different answers between Ben and the AI, we can notice that... Ben made a few mistakes!

For example, *El POLLO INKA* and *ELLA DINNING ROOM* should be restaurants. And *FAIRMONT BATTERY WHARF* should be an hotel.

For the rest, since we don't have the adresses, we'll give Ben the benefit of the doubt.

This means that the model got a accuracy rate greater than 94%! Not bad considering how simple our instructions were and how fast it got processed.

Depending on what you are working, maybe this is level of accuracy is enough if you provide it in your publication. One sure is certain though: doing it by hand would have been way slower (or just impossible it were millions of rows) and you would have made mistakes too!

If you are not satistied with the accuracy, you can tweak your prompt, give examples in your instructions, preprocess the data, or just switch to another model. But at least, with the accuracy, you won't be making changes blindly! You'll know when things get better and when they get worse.

![Testing the results.](/assets/simple-data-analysis/sda-and-ai/testing.png)

## Conclusion

Here, we classified data with a Gemini model. But you can do much more than that like cleaning, formatting and extracting data with LLMs!

And by using the SDA `aiRowByRow` method, you can do this easily, in a few lines of code, at scale!

If you have sensitive data that you don't want to send to Google or another AI provider, you can also use [Ollama](https://ollama.com/). Just start it and pass the following variables in your `.env` file. SDA will automatically switch to use the local model installed on your machine to crunch the data.

Sometimes, open-source models have more difficulties to directly return arrays (which is the expected returned response for `aiRowByRow`). If it's the case, I implemented the `clean` option that allow you to directly modify the LLM response, like restructuring it to an array.

```txt showLineNumbers filename="sda/main.ts"
OLLAMA=true
AI_MODEL=gemma3:4b
```

While amazing technology, remember that results are not guaranteed with LLMs. And, in my projects, the bigger the batches, the worst the results...

But still, they can boost you research and analysis. They are incredibly fast and easy to use.

So if you try them in one of your projects, [let me know](/contact)!

And thank you again to [Ben Welsh](https://github.com/palewire) and [Derek Willis](https://thescoop.org/about/) who created the Python version of this tutorial.

<NoticeEnd />
