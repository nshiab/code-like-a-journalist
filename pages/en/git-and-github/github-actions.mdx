---
title: Web scraping with GitHub actions  and workflows
description: In this lesson, we learn the basics of using GitHub actions and workflows, and we use them to scrape data.
---

import { Callout } from "nextra/components";
import { NoticeIntro, NoticeEnd } from "../../../components/Notices.jsx";

# Web scraping with GitHub actions and workflows

GitHub a platform to store your code, organize your work, collaborate with other fellow coders, but.. it's also a place where you can **run code**! ðŸ¤©

GitHub actions are mostly used to automatically run tests, builds and deployments on projects. But for data wranglers like us, it can also be used to scrape data.

In this lesson, we reuse the code fetching and extracting the latest temperature in Montreal from the previous lessons. But this time, we will use GitHub actions to trigger it every hour and accumulate the data, for free!

I strongly encourage you to complete the two previous lessons ([What is Git?](/git-and-github/git-basics) and [What is GitHub?](/git-and-github/github-basics)) before diving into this one. I'll assume you have a GitHub account and now the basic Git commands.

<NoticeIntro />

## Setup

Create a new folder and open it with VS Code.

In it, create a `main.ts` file with the code below. Here's what it does:
- It fetches the latest weather data in Montreal from the latest temperature in Montreal from the
[Meteorological Service of Canada API](https://eccc-msc.github.io/open-data/msc-data/obs_station/readme_obs_insitu_swobdatamart_en/).
- The data is in XML format. We use the library [`fast-xml-parser`](https://www.npmjs.com/package/fast-xml-parser) to convert it to JSON.
- Then we extract the temperature and the time from the nested JSON. We also create a variable with the current time of the scrape.
- Finally, we append the scraped data to `data.csv`.

Each time this scrip runs, a new file is added to the CSV. So the data is accumulated over time.

```ts showLineNumbers filename="main.ts"
import { XMLParser } from "fast-xml-parser";

const response = await fetch(
  "https://dd.weather.gc.ca/observations/swob-ml/latest/CWTQ-AUTO-minute-swob.xml",
);
const xml = await response.text();
const json = new XMLParser({ ignoreAttributes: false }).parse(xml);

const observation =
  json["om:ObservationCollection"]["om:member"]["om:Observation"];

const resultTime =
  observation["om:resultTime"]["gml:TimeInstant"]["gml:timePosition"];
console.log(resultTime);

const elements = observation["om:result"]["elements"]["element"];
type element = {
  "@_name": string;
  "@_value": string;
};
const temp = elements
  .find((d: element) => d["@_name"] === "air_temp")["@_value"];
console.log(temp);

const scrapeTime = new Date().toISOString();
console.log(scrapeTime);

await Deno.writeTextFile(
  "./data.csv",
  `\n${scrapeTime},${resultTime},${temp}`,
  {
    append: true,
  },
);
```

`main.ts` needs a `data.csv` file, so let's create one in the project.

```csv showLineNumbers filename="data.csv"
scrapeTime,resultTime,temp
```

We will push this project to GitHub. So let's add a `README.md` to the project to make it more presentable.

```md showLineNumbers filename="README.md"
# Montreal temperature scraper

Every hour, this project retrieves the latest temperature in Montreal
from the [Meteorological Service of Canada API](https://eccc-msc.github.io/open-data/msc-data/obs_station/readme_obs_insitu_swobdatamart_en/)
and appends a new row to the `data.csv` file.
```

Let's take this for a run!

First, install `fast-xml-parser`:
- `deno add npm:fast-xml-parser`

Then run `main.ts`:
- `deno -A main.ts`

And you should see the temperature being logged and a row added to the `data.csv` file!

![The README.](/assets/git-and-github/github-actions/setup.png)

## Creating a new GitHub repository

Let's initialize a new Git repository and push it to GitHub.

First, we initialize and make a first commit:
- `git init`
- `git add -A`
- `git commit -m "First commit"`

Then create a new repository on GitHub.

![New repository on GitHub.](/assets/git-and-github/github-actions/setup.png)

Give it a name and a description (optionnal). You can make it private or public, as you wish.

You don't need a `README` because we created one. You don't need a `.gitignore` and you don't need a license for now.

Then hit `Create repository`.

![Settings for a new repository on GitHub.](/assets/git-and-github/github-actions/repo-settings.png)

Copy the commands *to push an existing repository from the command line**.

![Commands to sync the new GitHub repository.](/assets/git-and-github/github-actions/commands.png)

And run these commands in your project terminal.

![Repository synced with the new GitHub repository.](/assets/git-and-github/github-actions/first-push.png)

Now, if you refresh your repository GitHub page, you'll see your code and `README.md` below it.

## Adding a workflow

While it's called *GitHub Actions*, you are not creating *actions* but **workflows**. (Why? Whyyyyyyy? ðŸ˜­). And you can see your workflows in the `Actions` section.

![The Actions tab in new GitHub repository.](/assets/git-and-github/github-actions/actions.png)

Right now, there is no workflows in our repository. GitHub suggests a few for us. Feel free to explore but we are going to add one manually.

![GitHub suggesting actions.](/assets/git-and-github/github-actions/no-actions.png)

For GitHub to automatically detect workflows in the repository, we need to create YAML files in `.github/workflows/`.

YAML stands for *YAML Ainâ€™t Markup Language* â€” but to me, it still feels a bit like markupâ€¦ just stricter. Indentation matters, for example. Itâ€™s very common for configuration files. It's not complicated and we will write some together, so don't worry about it.

Let's create new folders and a new file `.github/workflows/scrape.yml`. Let's keep it empty for now.

Commit it and push it to GitHub:
- `git add -A`
- `git commit -m "Adding scrape.yml"`
- `git push`

![An empyt .yml file.](/assets/git-and-github/github-actions/YML.png)

After a few seconds, if you refresh the Actions page of your repo, you'll see your workflow automatically detected by GitHub! But since the file is empty, it's failing.

![Failed workflow.](/assets/git-and-github/github-actions/failed.png)

## Writing a workflow

Let's do that step by step.

### Name and events

When you create a workflow, you first need to give a name and to decide when it should be triggered.

Below, we tell GitHub to trigger this workflow in three situation:
- `push` means when we push new code to the `main` branch. Note the indentation. It indicates hierarchy and grouping.
- `workflow_dispatch` means we will be able to click on a button to manually trigger the workflow. Always handy.
- `schedule` sets a **cron job**, which is a way to run scripts on a schedule. Here, the `0 * * * *` means the workflow will run at the start of every hour (minutes `0`).

```yml showLineNumbers filename=".github/workflows/scrape.yml" {1-9}
name: Scrape Montreal temperature

on:
  push:
    branches:
      - main
  workflow_dispatch:
  schedule:
    - cron: "0 * * * *"
```
<Callout type="info" emoji="ðŸ’¡">
   Cron jobs are widely used but they syntax is not intuitive. I usually use [crontab.guru](https://crontab.guru/) to make sense of them. Also, it's important to note that cron jobs in GitHub workflows are very accurate. When your workflow needs to run, GitHub pushes it to a queue and it waits for a virtual machine to be available to actually run it. So it might actually run a few minutes late. For most web scraping tasks, it's fine. But if you need to trigger critical scripts at very specific times, GitHub Actions might not be for you.
</Callout>

### Jobs

Now we need to describe the **jobs** we want the workflow to accomplish. You can have multiple jobs in a workflow. Here, we have just one. We name it `scrape`.

We tell GitHub we want it to run on a virtual machine with the latest Ubuntu distribution. This means it will run on a cheap virtual machine, which is perfectly fine for us. But you have [choices](https://docs.github.com/en/actions/writing-workflows/workflow-syntax-for-github-actions#choosing-github-hosted-runners), if you ever need something different.

Our `main.ts` script writes the freshly scraped data to `data.csv`, so we need to give the workflow the permission to write and modify the files of the repository.

```yml showLineNumbers filename=".github/workflows/scrape.yml" {11-15}
name: Scrape Montreal temperature

on:
  push:
    branches:
      - main
  workflow_dispatch:
  schedule:
    - cron: "0 * * * *"

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write
```

### Steps

The only thing left to do is to tell GitHub the **steps** of this job. Each step has a name:
- `Checking out the repo` tells the workflow to actually get in the repository. We use the precoded `actions/checkout@v4` for that. There are a lot of actions available for you to use, especially for common tasks. It's nice to not have to code everything yourself! ðŸ¥³
- `Set up Deno` installs Deno. We use an available action for that too. We specify that we want the v2 of Deno.
- `Run main.ts` runs our script, like we would manually.

```yml showLineNumbers filename=".github/workflows/scrape.yml" {16-26}
name: Scrape Montreal temperature

on:
  push:
    branches:
      - main
  workflow_dispatch:
  schedule:
    - cron: "0 * * * *"

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checking out the repo
        uses: actions/checkout@v4
      - name: Set up Deno
        uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x
      - name: Run main.ts
        run: deno -A main.ts
```

### Saving the data

Now, if you save your workflow and push it to GitHub, it will fetch the data and write it to `data.csv` but... it won't **actually save `data.csv`**!

Remember: this is running on a virtual machine. So it runs, writes a new row to `data.csv` but then it's destroyed and removed. So the data saved in memory disappears...

This is why we have a last step to **commit the changes**. We create a Git user named `Automated` that commits and pushes `data.csv` file changes! Isn't that brilliant? ðŸ™Œ

```yml showLineNumbers filename=".github/workflows/scrape.yml" {25-26}
name: Scrape Montreal temperature

on:
  push:
    branches:
      - main
  workflow_dispatch:
  schedule:
    - cron: "0 * * * *"

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checking out the repo
        uses: actions/checkout@v4
      - name: Set up Deno
        uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x
      - name: Run main.ts
        run: deno -A main.ts
      - name: Commit changes
        run: git config user.name "Automated" && git config user.email "actions@users.noreply.github.com" && git add data.csv && git commit -m "Automated update" && git push
```

Save the file and push it to GitHub:
- `git add -A`
- `git commit -m "Updating workflow`
- `git push`

And now if you refresh your repository Actions page, you'll see your workflow properly running!

![Success workflow.](/assets/git-and-github/github-actions/success.png)

If you click on you it, you see the jobs. Here, there's just one: `scrape`.

![Success job.](/assets/git-and-github/github-actions/success-jobs.png)

And if you click on the job, you'll see its steps and can click on them. This is useful when something is failing. You'll be able to see the error messages here.

![Success steps.](/assets/git-and-github/github-actions/success-steps.png)

If you go back to the home page of the repository, you'll that the lastest commit has been done by the `Automated` Git user we set up in the workflow.

![Automated commit.](/assets/git-and-github/github-actions/automated-commit.png)

And if you check the `data.csv` file, you'll see a second row has been added!

![The updated data.csv file.](/assets/git-and-github/github-actions/data.csv.png)

You can even go one step further and check the commit to see the actual change.

By the way, this is really great way to check what changed in web pages, data files, and more. If you scrape entire files, GitHub will be able to tell you what changed and when. You'll have a full detailed history thanks to commits.

![The detailed commit.](/assets/git-and-github/github-actions/commit.png)

This has run before we pushed on `main`. But now, it will be updating automatically every hour thanks to our cron job!

Before GitHub actions, you would have needed to pay and set up a server to scrape this information on a schedule. But now, you can do it for free with only one simple YAML file! ðŸ’ƒðŸ•º

## Retrieving the data

The comitted data is on Github. To retrieve it in your local repo, just `pull` it:
- `git pull`

Note that I went for lunch before taking this screenshot. So I have three lines! The scraper ran while I was away! ðŸ˜„

![The pulled data.](/assets/git-and-github/github-actions/pull.png)

If your repository is public, you can also fetch the CSV data easily. This is useful when you want to work with up-to-data data in another project. Click on the `Raw` button to have a URL to the actual file.

![The link to the raw CSV file.](/assets/git-and-github/github-actions/raw.png)

Now you can directly fetch this URL.

![The link to the raw data.](/assets/git-and-github/github-actions/url.png)

You can use this URL to fetch the latest data and process it, for example with the [Simple Data Analysis library](https://github.com/nshiab/simple-data-analysis).

```ts showLineNumbers
import { SimpleDB } from "@nshiab/simple-data-analysis";

const sdb = new SimpleDB();

const temperatures = sdb.newTable();
await temperatures.loadData(
  "https://raw.githubusercontent.com/nshiab/temperature-scraper/refs/heads/main/data.csv",
);
await temperatures.logTable();

await sdb.done();
```
![Fetching the data with the Simple Data Analysis.](/assets/git-and-github/github-actions/fetch-test.png)

## Manually triggering a workflow

Right now, our scrape is triggered when we push new code on `main` and every hour, at the top of the hour.

But since we added the `workflow_dispatch` in our configuration, we can also trigger it manually by clicking on it on this Actions page.

![Manually triggering the workflow.](/assets/git-and-github/github-actions/manual.png)

## Disabling a workflow

To remove a workflow, you need to actually remove the YAML file from the repository and commit/push the change.

But sometimes, you just want to temporarly disable it, for example while fixing something in the code, just because you don't need the data for the moment, or because you hit the limit of your free GitHub account! ðŸ˜¬

To disable a workflow while keeping the configuration file in the repository,

![Disabling a workflow.](/assets/git-and-github/github-actions/disable.png)

## Conclusion

Congratulations! You know now our to create simple workflows. You can use that to scrape data but also for so many things! This is code, so possibility are endless!

I hope you found this lesson interesting. But we haven't explored everything GitHub has to offer... In the next lesson, we will discover Pages, which allow you to create websites for... wait for it... free! And since you now know about GitHub actions, you'll be able to republish your webpages automatically each time you push code to the `main` branch. ðŸ¤“

See you for the next lesson!